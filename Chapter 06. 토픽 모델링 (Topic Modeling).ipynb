{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e65de0c",
   "metadata": {},
   "source": [
    "## 잠재 의미 분석 (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309032c",
   "metadata": {},
   "source": [
    "### Trunced STV 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359713a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:27:41.318230Z",
     "start_time": "2021-11-28T07:27:41.308005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c69eed2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:30:55.345753Z",
     "start_time": "2021-11-28T07:30:55.342009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n",
      "[2.68731789 2.04508425 1.73205081 0.77197992]\n"
     ]
    }
   ],
   "source": [
    "U, s, VT = np.linalg.svd(A, full_matrices = True)\n",
    "\n",
    "print(U.round(2))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbcf6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:30:33.540837Z",
     "start_time": "2021-11-28T07:30:33.537396Z"
    }
   },
   "source": [
    "s가 대각행렬로 안나오고 singular value만 나왔다. 다시 대각행렬로 바꿔주면,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e73d255a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:32:00.509162Z",
     "start_time": "2021-11-28T07:32:00.505601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "S = np.zeros((4, 9))\n",
    "S[:4, :4] = np.diag(s)\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "707b3048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:34:16.895268Z",
     "start_time": "2021-11-28T07:34:16.890871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# t=2로 설정하고 trunced SVD를 수행하면\n",
    "\n",
    "S = S[:2, :2]\n",
    "U = U[:, :2]\n",
    "VT = VT[:2, :]\n",
    "\n",
    "A_prime = np.dot(np.dot(U, S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d414248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:35:34.255757Z",
     "start_time": "2021-11-28T07:35:34.252461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape # 4개의 문장을 각각 2개의 값으로 표현했다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c0d1a86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:35:31.471417Z",
     "start_time": "2021-11-28T07:35:31.467677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT.shape # 9개의 keyword를 2개의 값으로 표현했다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81173597",
   "metadata": {},
   "source": [
    "### 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c517766e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:38:02.006381Z",
     "start_time": "2021-11-28T07:37:12.282194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1677c8c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:38:10.566172Z",
     "start_time": "2021-11-28T07:38:10.562860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45547802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T07:38:36.000751Z",
     "start_time": "2021-11-28T07:38:35.997173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc533727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:06:18.557352Z",
     "start_time": "2021-11-28T08:06:17.511997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ph/pv7rmwrx0yl4zl9pm78d51hm0000gn/T/ipykernel_50234/2086978340.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein israeli journalist will speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree home runs clemens always memorable kinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet with orange micros grappler syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>argument with murphy scared hell when came las...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure about story seem biased what disagre...  \n",
       "1      yeah expect people read actually accept hard a...  \n",
       "2      although realize that principle your strongest...  \n",
       "3      notwithstanding legitimate fuss about this pro...  \n",
       "4      well will have change scoring playoff pool unf...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein israeli journalist will speak...  \n",
       "11310                                                     \n",
       "11311  agree home runs clemens always memorable kinda...  \n",
       "11312  used deskjet with orange micros grappler syste...  \n",
       "11313  argument with murphy scared hell when came las...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 전처리\n",
    "news_df = pd.DataFrame({'document': documents})\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ee55fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:08:10.957141Z",
     "start_time": "2021-11-28T08:08:08.535136Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "156798ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:08:12.275491Z",
     "start_time": "2021-11-28T08:08:12.268698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased, disagree, st...\n",
       "1        [yeah, expect, people, read, actually, accept,...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4        [well, change, scoring, playoff, pool, unfortu...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein, israeli, journalist, speak...\n",
       "11310                                                   []\n",
       "11311    [agree, home, runs, clemens, always, memorable...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [argument, murphy, scared, hell, came, last, y...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1807b5d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:09:18.801286Z",
     "start_time": "2021-11-28T08:09:18.759826Z"
    }
   },
   "outputs": [],
   "source": [
    "detokenized_doc = [' '.join(x) for x in tokenized_doc]\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851c813",
   "metadata": {},
   "source": [
    "TF-IDF 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c31901e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:12:55.552860Z",
     "start_time": "2021-11-28T08:12:54.707144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "                             max_df = 0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cfa1267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:12:55.810328Z",
     "start_time": "2021-11-28T08:12:55.807273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12383418, 0.15794161, 0.11513969, ..., 0.08171431, 0.12486897,\n",
       "       0.1573019 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5286f93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:20:18.040975Z",
     "start_time": "2021-11-28T08:20:17.191759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20,\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=100,\n",
    "                         random_state=122)\n",
    "svd_model.fit(X)\n",
    "print(len(svd_model.components_))\n",
    "# svd_model.components_ = VT 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b03266b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:13:26.345840Z",
     "start_time": "2021-11-28T08:13:26.342130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf2e1c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:17:16.686694Z",
     "start_time": "2021-11-28T08:17:16.677259Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timber/opt/anaconda3/envs/NLP_tf/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "# 각 20개의 행의 각 1,000개 열 중 가장 큰 5개의 값을 찾아서 단어로 출력\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(f\"Topic {idx+1}:\", \n",
    "              [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "# 부가설명을 하자면, argsort()는 정렬했을 때 나오는 index의 순서를 반환한다.\n",
    "# 즉, VT에서 각 20개의 topic 별로 1000개의 단어들 중에서 가장 성적이 높은 단어 5개의 index 값을 받고, \n",
    "# 단어 - 점수 를 매칭해서 보여주는거다.\n",
    "\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff54c08b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T08:23:37.136763Z",
     "start_time": "2021-11-28T08:23:37.134094Z"
    }
   },
   "outputs": [],
   "source": [
    "components = svd_model.components_[0]\n",
    "topic = components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7d26c",
   "metadata": {},
   "source": [
    "## 잠재 디리클레 할당 (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d250d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:NLP_tf] *",
   "language": "python",
   "name": "conda-env-NLP_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
